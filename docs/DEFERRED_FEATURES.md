# Deferred Features

This document explicitly lists features that are **NOT** in v2.0.0 and will not be added without a version bump. This prevents scope creep and sets clear expectations.

## Explicitly Deferred to v2.x / v3

### ⏳ Incremental Text Streaming

**What it is**: Word-by-word speculative synthesis with rollback capability.

**Current state**: v2 has sentence-level streaming. The compiler waits for sentence boundaries (`. ! ?`) before emitting graphs.

**Why deferred**: True incremental streaming requires:
- Speculative execution (synthesize partial sentences)
- Rollback on correction
- Complex cache invalidation

**Timeline**: Potentially v2.1 or v2.2, pending design review.

---

### ⏳ Native Paralinguistics in All Backends

**What it is**: Natural-sounding laughs, sighs, breaths generated by the TTS model itself.

**Current state**: v2 supports paralinguistic *events* via pre-recorded WAV insertion. The audio quality depends on asset quality, not the TTS model.

**Why deferred**: Requires backend-specific support:
- Kokoro: Would need special token training
- Piper: Architecture doesn't support it
- Future backends (Chatterbox, F5-TTS): May have native support

**Timeline**: v3, when backends with native support are integrated.

---

### ⏳ True PCM Mixing / Sidechain Compression

**What it is**: Real audio mixing where event audio and speech audio overlap in time, with proper gain management.

**Current state**: v2 ducking applies gain envelopes to sequential audio. Events and speech never actually overlap in the timeline.

**Why deferred**: Requires:
- Audio engine (separate from TTS engine)
- Buffer management for look-ahead
- DSP complexity inappropriate for v2 scope

**Timeline**: v3, potentially as a separate audio post-processing module.

---

### ⏳ Real-Time Voice Cloning

**What it is**: Clone a voice from a short audio sample and use it for synthesis.

**Current state**: v2 architecture supports voice cloning via `SpeakerRef.from_embedding()`, but:
- No embedding extraction is implemented
- No trained cloning models are bundled
- The boundary exists (raw audio never reaches engine)

**Why deferred**: Voice cloning requires:
- Embedding extraction model
- Quality/fidelity validation
- Ethical considerations and guidelines

**Timeline**: v2.x or v3, pending model availability and policy decisions.

---

### ⏳ DSP-Heavy Effects

**What it is**: Real-time effects like reverb, EQ, pitch shifting, time stretching.

**Current state**: v2 outputs clean PCM. All effects would need to be applied externally.

**Why deferred**: These are post-processing concerns, not TTS concerns. Adding them would blur the architecture boundary.

**Timeline**: Never in voice-soundboard itself. Use a DSP library instead.

---

## What IS in v2.0.0

For clarity, here's what v2 *does* include:

✅ **Compiler/Engine separation** - Clean architecture
✅ **Two validated backends** - Kokoro (24kHz), Piper (22kHz)
✅ **Paralinguistic events** - WAV insertion at timeline boundaries
✅ **Incremental compiler** - Sentence-level streaming
✅ **Audio event adapter** - Manifest-driven asset management
✅ **Ducking** - Gain envelopes (no overlap)
✅ **Property-based testing** - Timing invariant guarantees
✅ **Architecture enforcement** - CI validates invariants

---

## How to Request a Deferred Feature

1. Check if it's listed here
2. If yes: Wait for the indicated timeline, or contribute a design proposal
3. If no: Open an issue with a detailed use case

We prioritize features based on:
- Architectural fit (does it preserve invariants?)
- User demand (how many people need it?)
- Implementation complexity (can it be done cleanly?)
